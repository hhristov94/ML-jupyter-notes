{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_output(weights, bias, x):\n",
    "    \"\"\"returns 1 if the perceptron fires, 0 if not\"\"\"\n",
    "    calculation = np.dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration(x,w,b):\n",
    "    weighted_sum = sum(x[k] * w[k] for k in xrange(0,len(x)))\n",
    "    return weighted_sum + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude(x):\n",
    "    return sum(k**2 for k in x)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(point,step_size,threshold):\n",
    "    value = f(point)\n",
    "    new_point = point - step_size * gradient(point)\n",
    "    new_value = f(new_point)\n",
    "    if abs(new_value - value) < threshold:\n",
    "        return value\n",
    "    return gradient_descent(new_point,step_size,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##AND GATE\n",
    "weights = [2, 2]\n",
    "bias = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OR GAte\n",
    "weights = [2, 2]\n",
    "bias = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [-2]\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1]\n",
      "[1]\n",
      "[ 0 -1]\n",
      "[-1]\n",
      "[10  1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.array([0,0])\n",
    "b = np.array([0])\n",
    "\n",
    "x1 = np.array([-1,1])\n",
    "y1 = np.array([1])\n",
    "\n",
    "x2 = np.array([0,-1])\n",
    "y2 = np.array([-1])\n",
    "\n",
    "x3 = np.array([10,1])\n",
    "y3 = np.array([1])\n",
    "\n",
    "x = np.vstack((x1,x2,x3))\n",
    "y = np.vstack((y1,y2,y3))\n",
    "\n",
    "#p.apply_along_axis( myfunction, axis=1, arr=mymatrix )\n",
    "for x_i, y_i in zip(x,y):\n",
    "    print(x_i)\n",
    "    print(y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "while counter < len(y):\n",
    "    for x_i, y_i in zip(x,y):\n",
    "        o = perceptron_output(w,b,x_i)\n",
    "        if o == 0:\n",
    "            w = w + y_i*x_i\n",
    "            b = b + y_i\n",
    "        else: \n",
    "            counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Updating weights\n",
      "0\n",
      "Updating weights\n",
      "0\n",
      "Updating weights\n",
      "0\n",
      "Updating weights\n",
      "0\n",
      "Updating weights\n",
      "1\n",
      "0\n",
      "Updating weights\n",
      "0\n",
      "Updating weights\n",
      "1\n",
      "1\n",
      "0\n",
      "Updating weights\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "while counter < len(y):\n",
    "    for x_i, y_i in zip(x,y):\n",
    "        o = perceptron_output(w,b,x_i)\n",
    "        print(o)\n",
    "        if o==0:\n",
    "            print(\"Updating weights\")\n",
    "            w = w + y_i*x_i\n",
    "            b = b + y_i\n",
    "        else:\n",
    "            counter +=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_output(w,b,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sizes = [2,3,1]\n",
    "num_layers = len(sizes)#3\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "def feedforward(a):\n",
    "    for b,w in zip(biases, weights):\n",
    "        a = sigmoid(np.dot(w,a)+ b)\n",
    "    return a\n",
    "\n",
    "def SGD(training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    n = len(training_data)\n",
    "    for j in xrange(epochs):\n",
    "        np.random.shuffle(training_data)\n",
    "        mini_batches = [\n",
    "            training_data[k:k+mini_batch_size]\n",
    "            for k in range(0, n, mini_batch_size)]\n",
    "        for mini_batch in mini_batches:\n",
    "            update_mini_batch(mini_batch, eta)\n",
    "        print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "def update_mini_batch(mini_batch, eta):\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    for x,y in mini_batch:\n",
    "        delta_nabla_n, delta_nabla_w = backprop(x,y)\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, nabla_w)]\n",
    "    biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, nabla_b)]\n",
    "\n",
    "def backprop(x,y):\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    activation = x\n",
    "    activations = [x]\n",
    "    zs = []\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    for l in range(2,num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (nabla_b, nabla_w)\n",
    "\n",
    "def cost_derivative(output_activations,y):\n",
    "    return (output_acctivations-y)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
